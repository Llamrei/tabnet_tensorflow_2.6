{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import common tensorflow layers and activations\n",
    "# Import tensor kerastensor\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Layer\n",
    "from tensorflow.keras.layers import Lambda, Multiply, Add \n",
    "from tensorflow.keras.activations import relu, sigmoid, softmax\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentiveTransformer(Layer):\n",
    "    def __init__(self, units, priors = None, gamma=1.3, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.units = units\n",
    "        self.priors = priors if priors is not None else []\n",
    "        self.gamma = tf.constant(gamma, dtype=tf.float32)\n",
    "        # self.forward = None\n",
    "        self.dense = Dense(self.units, name=f\"{self.name}_dense\")\n",
    "        self.BN = BatchNormalization(name=f\"{self.name}_BN\")\n",
    "\n",
    "    \n",
    "    # def build(self, input_shape):\n",
    "    #     # super().build(input_shape)\n",
    "    #     _in = Input(shape=input_shape[1:], name=f\"{self._name}_input\")\n",
    "    #     x = Dense(self.units, name=f\"{self._name}_dense\")(_in)\n",
    "    #     x = BatchNormalization(name=f\"{self._name}_BN\")(x)\n",
    "    #     print(\"building\", self.name)\n",
    "    #     for prior in self.priors:\n",
    "    #         print(\"prior_mul\", prior._keras_history.layer.name)\n",
    "    #         x = Multiply()([(self.gamma - prior),x])\n",
    "    #     x = softmax(x, axis=-1)\n",
    "    #     self.forward = Model(_in, x)\n",
    "    \n",
    "    def call(self, data):\n",
    "        candidate_mask = self.BN(self.dense(data))\n",
    "        print(self.priors)\n",
    "        for prior in self.priors:\n",
    "            print(prior)\n",
    "            candidate_mask = candidate_mask * (self.gamma - prior)\n",
    "        return candidate_mask\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListWrapper([])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 7), dtype=float32, numpy=\n",
       " array([[-3.7246137 , -1.2955515 ,  1.0983032 , -1.7555252 ,  0.35104498,\n",
       "         -2.64866   ,  0.9813243 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
       " array([[-2.088805  ,  2.7135048 , -0.96651167, -3.3743446 , -2.7218435 ,\n",
       "         -2.6411772 ,  1.7550788 ,  0.5382888 , -0.23096025, -2.237022  ]],\n",
       "       dtype=float32)>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test AttentiveTransformer on normal data with empty list of priors\n",
    "at = AttentiveTransformer(units=10, priors=[])\n",
    "x = tf.random.normal((1, 7))\n",
    "y = at(x)\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListWrapper([])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
       "array([[-2.088805  ,  2.7135048 , -0.96651167, -3.3743446 , -2.7218435 ,\n",
       "        -2.6411772 ,  1.7550788 ,  0.5382888 , -0.23096025, -2.237022  ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test AttentiveTransformer on normal data with instance of itself as prior\n",
    "at2 = AttentiveTransformer(units=10, priors=[y, 1])\n",
    "z = at(x)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListWrapper([])\n"
     ]
    }
   ],
   "source": [
    "# Test graph construction of AttentiveTransformer with no prior\n",
    "test_in = Input(shape=(7,), name=\"test_in\", dtype=\"float32\")\n",
    "at1 = AttentiveTransformer(units=7, priors=[], name=\"test_attn_1\")\n",
    "y = at1(test_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListWrapper([])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x7f0e8d7ea9d0>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "at2 = AttentiveTransformer(units=7, name=\"test_attn_2\")\n",
    "x = at2(test_in)*(1.3-y)\n",
    "Model(test_in, x*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabnet",
   "language": "python",
   "name": "tabnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
