{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# from tensorflow.python.framework.ops import disable_eager_execution \n",
    "# disable_eager_execution()\n",
    "# from tensorflow.python.framework.ops import enable_eager_execution\n",
    "# enable_eager_execution()\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import common tensorflow layers and activations\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Layer\n",
    "from tensorflow.keras.layers import Lambda, Multiply, Add, Rescaling\n",
    "from tensorflow.keras.activations import relu, sigmoid, softmax\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "from tabnet import TabNet\n",
    "from tabnet import TabNetClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabnet experiment Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16384\n",
    "LAMBDA = 0.0001\n",
    "N_A = 64\n",
    "N_D = 64\n",
    "VIRTUAL_BATCH_SIZE = 512\n",
    "BATCH_MOMENTUM = 0.7\n",
    "N_STEPS = 5\n",
    "GAMMA = 1.5\n",
    "LEARNING_RATE = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    0.02,\n",
    "    decay_steps=500,\n",
    "    decay_rate=0.95\n",
    "    )\n",
    "OPTIMIZER = tf.keras.optimizers.Adam(LEARNING_RATE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Defn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_302331/3591880323.py:50: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train[..., np.newaxis]))\n",
      "2023-05-11 13:09:04.685081: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-05-11 13:09:04.685112: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (al3615): /proc/driver/nvidia/version does not exist\n",
      "2023-05-11 13:09:04.685487: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/tmp/ipykernel_302331/3591880323.py:52: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val[..., np.newaxis]))\n",
      "/tmp/ipykernel_302331/3591880323.py:54: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test[..., np.newaxis]))\n"
     ]
    }
   ],
   "source": [
    "# Load forest_cov data\n",
    "forest_cov = pd.read_csv(\"./covtype.data\")\n",
    "float_cols = ['Elevation', 'Aspect', 'Slope', 'Hydrology_X', 'Hydrology_Y',\n",
    "    'Roadways_X', 'Hillshade_9am', 'Hillshade_noon', 'Hillshade_3pm',\n",
    "    'Firepoints_X']\n",
    "bool_cols = [\n",
    "    *[f\"Wilderness_Area_{i}\" for i in range(4)],\n",
    "    *[f\"Soil_Type_{i}\" for i in range(40)]\n",
    "    ]\n",
    "response_cols = [\n",
    "    'Cover_Type'\n",
    "]\n",
    "forest_cov.columns = [\n",
    "    *float_cols,\n",
    "    *bool_cols,\n",
    "    *response_cols\n",
    "    ]\n",
    "outputs = ['Cover_Type']\n",
    "response = 'Cover_Type'\n",
    "forest_cov.drop([x for x in outputs if x != response], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Split data into train and test\n",
    "data_train, data_test = train_test_split(forest_cov, test_size=0.3, random_state=42)\n",
    "data_val, data_test = train_test_split(data_test, test_size=0.33, random_state=42)\n",
    "\n",
    "# Split into X and y\n",
    "X_train = data_train.drop(response, axis=1, errors='ignore')\n",
    "y_train = data_train[response] - 1\n",
    "X_val = data_val.drop(response, axis=1, errors='ignore')\n",
    "y_val = data_val[response] - 1\n",
    "X_test = data_test.drop(response, axis=1, errors='ignore')\n",
    "y_test = data_test[response] - 1\n",
    "\n",
    "# Fillna\n",
    "for col in X_train.columns:\n",
    "    if X_train[col].isna().sum() > 0:\n",
    "        print(f\"Column {col} has {X_train[col].isna().sum()} missing values\")\n",
    "    X_train[col].fillna(X_train[col].mean(), inplace=True)\n",
    "    X_val[col].fillna(X_train[col].mean(), inplace=True)\n",
    "    X_test[col].fillna(X_train[col].mean(), inplace=True)\n",
    "\n",
    "# Scale input data - save scaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Make tensorflow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train[..., np.newaxis]))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=4048).batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val[..., np.newaxis]))\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test[..., np.newaxis]))\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((581011, 55),\n",
       " Index(['Elevation', 'Aspect', 'Slope', 'Hydrology_X', 'Hydrology_Y',\n",
       "        'Roadways_X', 'Hillshade_9am', 'Hillshade_noon', 'Hillshade_3pm',\n",
       "        'Firepoints_X', 'Wilderness_Area_0', 'Wilderness_Area_1',\n",
       "        'Wilderness_Area_2', 'Wilderness_Area_3', 'Soil_Type_0', 'Soil_Type_1',\n",
       "        'Soil_Type_2', 'Soil_Type_3', 'Soil_Type_4', 'Soil_Type_5',\n",
       "        'Soil_Type_6', 'Soil_Type_7', 'Soil_Type_8', 'Soil_Type_9',\n",
       "        'Soil_Type_10', 'Soil_Type_11', 'Soil_Type_12', 'Soil_Type_13',\n",
       "        'Soil_Type_14', 'Soil_Type_15', 'Soil_Type_16', 'Soil_Type_17',\n",
       "        'Soil_Type_18', 'Soil_Type_19', 'Soil_Type_20', 'Soil_Type_21',\n",
       "        'Soil_Type_22', 'Soil_Type_23', 'Soil_Type_24', 'Soil_Type_25',\n",
       "        'Soil_Type_26', 'Soil_Type_27', 'Soil_Type_28', 'Soil_Type_29',\n",
       "        'Soil_Type_30', 'Soil_Type_31', 'Soil_Type_32', 'Soil_Type_33',\n",
       "        'Soil_Type_34', 'Soil_Type_35', 'Soil_Type_36', 'Soil_Type_37',\n",
       "        'Soil_Type_38', 'Soil_Type_39', 'Cover_Type'],\n",
       "       dtype='object'),\n",
       " 1    581011\n",
       " dtype: int64,\n",
       " 1    581011\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_cov.shape, forest_cov.columns, forest_cov[[f\"Wilderness_Area_{i}\" for i in range(4)]].sum(axis=1).value_counts(), forest_cov[[f\"Soil_Type_{i}\" for i in range(40)]].sum(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(16384, 54), dtype=tf.float64, name=None),\n",
       " TensorSpec(shape=(16384, 1), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.element_spec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model creation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabnet = TabNet(\n",
    "    dim_features=X_train.shape[1],\n",
    "    dim_attention=128,\n",
    "    dim_output=64,\n",
    "    sparsity=0.0001,\n",
    "    num_steps=5,\n",
    "    gamma=1.5,\n",
    "    feature_shared_layers=2,\n",
    "    feature_transformer_layers=2,\n",
    "    output_activation=None\n",
    "    )\n",
    "\n",
    "tabnet.compile( \n",
    "    optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            initial_learning_rate=0.02,\n",
    "            decay_steps=500,\n",
    "            decay_rate=0.95\n",
    "        ), \n",
    "    ), \n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "    ]\n",
    ")\n",
    "tabnet.build(X_train[:1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tab_net\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "shared_feature_layer (Shared multiple                  90624     \n",
      "_________________________________________________________________\n",
      "feat_0 (FeatureTransformer)  multiple                  190720    \n",
      "_________________________________________________________________\n",
      "feat_1 (FeatureTransformer)  multiple                  190720    \n",
      "_________________________________________________________________\n",
      "feat_2 (FeatureTransformer)  multiple                  190720    \n",
      "_________________________________________________________________\n",
      "feat_3 (FeatureTransformer)  multiple                  190720    \n",
      "_________________________________________________________________\n",
      "feat_4 (FeatureTransformer)  multiple                  190720    \n",
      "_________________________________________________________________\n",
      "feat_5 (FeatureTransformer)  multiple                  190720    \n",
      "_________________________________________________________________\n",
      "attn_1 (AttentiveTransformer multiple                  3726      \n",
      "_________________________________________________________________\n",
      "attn_2 (AttentiveTransformer multiple                  3726      \n",
      "_________________________________________________________________\n",
      "attn_3 (AttentiveTransformer multiple                  3726      \n",
      "_________________________________________________________________\n",
      "attn_4 (AttentiveTransformer multiple                  3726      \n",
      "_________________________________________________________________\n",
      "attn_5 (AttentiveTransformer multiple                  3726      \n",
      "_________________________________________________________________\n",
      "norm_in (BatchNormalization) multiple                  216       \n",
      "_________________________________________________________________\n",
      "output (Dense)               multiple                  455       \n",
      "=================================================================\n",
      "Total params: 710,501\n",
      "Trainable params: 706,269\n",
      "Non-trainable params: 4,232\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tabnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TabNet]: 64 features will be used for decision steps.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 17:56:01.339649: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 175697424 exceeds 10% of free system memory.\n",
      "2023-05-05 17:56:01.983108: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(16384, 7), dtype=float32, numpy=\n",
       "array([[0.12565674, 0.16860135, 0.14593603, ..., 0.1298019 , 0.11464456,\n",
       "        0.14293875],\n",
       "       [0.14519764, 0.14022234, 0.14879064, ..., 0.13947473, 0.14394005,\n",
       "        0.1384698 ],\n",
       "       [0.14177747, 0.1417857 , 0.14354606, ..., 0.14288153, 0.14341688,\n",
       "        0.14415672],\n",
       "       ...,\n",
       "       [0.14239503, 0.14264832, 0.14400764, ..., 0.14283682, 0.14187367,\n",
       "        0.14437875],\n",
       "       [0.14314249, 0.14363249, 0.14279087, ..., 0.14159256, 0.14391036,\n",
       "        0.14236593],\n",
       "       [0.1424368 , 0.14261298, 0.14288384, ..., 0.14241643, 0.14342001,\n",
       "        0.14298274]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tabnet\n",
    "from importlib import reload\n",
    "reload(tabnet)\n",
    "\n",
    "from tabnet import TabNetClassifier\n",
    "\n",
    "online_implementation = TabNetClassifier(\n",
    "    feature_columns=None,\n",
    "    num_classes=7,\n",
    "    output_dim=64,\n",
    "    feature_dim=128,\n",
    "    num_features=X_train.shape[1],\n",
    "    num_decision_steps=5,\n",
    "    relaxation_factor=1.5,\n",
    "    sparsity_coefficient=0.0001,\n",
    "    virtual_batch_size=512,\n",
    "    norm_type=\"batch\",\n",
    "    batch_momentum=0.7,\n",
    ")\n",
    "\n",
    "online_implementation.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            initial_learning_rate=0.02,\n",
    "            decay_steps=500,\n",
    "            decay_rate=0.95\n",
    "        ),\n",
    "    ),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "    ]\n",
    ")\n",
    "online_implementation(next(iter(train_dataset))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 17:56:02.710597: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 175697424 exceeds 10% of free system memory.\n",
      "2023-05-05 17:56:02.790012: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 175697424 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/130000\n",
      "24/25 [===========================>..] - ETA: 2s - loss: 0.9407 - accuracy: 0.6292"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " Input to reshape is a tensor with 3453696 values, but the requested shape requires a multiple of 131072\n\t [[node tab_net_classifier/tab_net_1/transform_block/transformblock_bn_f1/Reshape (defined at home/alexander/projects/deep_learning_vs_gbdt/venv/lib/python3.9/site-packages/tabnet/tabnet.py:34) ]] [Op:__inference_train_function_15214]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node tab_net_classifier/tab_net_1/transform_block/transformblock_bn_f1/Reshape:\n tab_net_classifier/tab_net_1/transform_block/transformblock_dense_f1/MatMul (defined at home/alexander/projects/deep_learning_vs_gbdt/venv/lib/python3.9/site-packages/tabnet/tabnet.py:33)\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history_online \u001b[39m=\u001b[39m online_implementation\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      2\u001b[0m     train_dataset,\n\u001b[1;32m      3\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m130000\u001b[39;49m,\n\u001b[1;32m      4\u001b[0m     validation_data\u001b[39m=\u001b[39;49mval_dataset,\n\u001b[1;32m      5\u001b[0m     \u001b[39m# callbacks=[early_stopping, lr_on_plateau]\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m )\n",
      "File \u001b[0;32m~/projects/deep_learning_vs_gbdt/venv/lib/python3.9/site-packages/keras/engine/training.py:1184\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1178\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1179\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   1180\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   1181\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1182\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   1183\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1184\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1185\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1186\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/projects/deep_learning_vs_gbdt/venv/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    882\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    884\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 885\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    887\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    888\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/projects/deep_learning_vs_gbdt/venv/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:917\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    914\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    915\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 917\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    918\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    919\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    920\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    921\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/projects/deep_learning_vs_gbdt/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py:3039\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3036\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   3037\u001b[0m   (graph_function,\n\u001b[1;32m   3038\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   3040\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/projects/deep_learning_vs_gbdt/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1963\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1960\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1961\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1962\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1963\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1964\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1965\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m     args,\n\u001b[1;32m   1967\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1968\u001b[0m     executing_eagerly)\n\u001b[1;32m   1969\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/projects/deep_learning_vs_gbdt/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py:591\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    590\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 591\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    592\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    593\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    594\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    595\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    596\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    597\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    598\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    599\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    600\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    603\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    604\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/projects/deep_learning_vs_gbdt/venv/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 59\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     60\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     62\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  Input to reshape is a tensor with 3453696 values, but the requested shape requires a multiple of 131072\n\t [[node tab_net_classifier/tab_net_1/transform_block/transformblock_bn_f1/Reshape (defined at home/alexander/projects/deep_learning_vs_gbdt/venv/lib/python3.9/site-packages/tabnet/tabnet.py:34) ]] [Op:__inference_train_function_15214]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node tab_net_classifier/tab_net_1/transform_block/transformblock_bn_f1/Reshape:\n tab_net_classifier/tab_net_1/transform_block/transformblock_dense_f1/MatMul (defined at home/alexander/projects/deep_learning_vs_gbdt/venv/lib/python3.9/site-packages/tabnet/tabnet.py:33)\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history_online = online_implementation.fit(\n",
    "    train_dataset,\n",
    "    epochs=130000,\n",
    "    validation_data=val_dataset,\n",
    "    # callbacks=[early_stopping, lr_on_plateau]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early_stopping = tf.keras.callbacks.EarlyStopping(patience=30, restore_best_weights=True, monitor=\"val_auc\")\n",
    "# lr_on_plateau = tf.keras.callbacks.ReduceLROnPlateau(patience=3, factor=0.5, min_lr=1e-5)\n",
    "history = tabnet.fit(\n",
    "    train_dataset, \n",
    "    epochs=130000, \n",
    "    validation_data=val_dataset, \n",
    "    # callbacks=[early_stopping, lr_on_plateau]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabnet.history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot history loss and RMSE for training and validation set; train solid line, validation dashed line\n",
    "fig, (top_ax, bottom_ax) = plt.subplots(2, 1, figsize=(10, 10), sharex=True)\n",
    "\n",
    "hist = tabnet.history.history\n",
    "\n",
    "top_ax.plot(hist['loss'], label='train_loss', c='b')\n",
    "top_ax.plot(hist['val_loss'], label='val_loss', linestyle='--', c='b')\n",
    "second_ax = top_ax.twinx()\n",
    "# Plot precision and recall on second axis in orange and red respectively\n",
    "second_ax.plot(hist['precision'], label='train_precision', c='orange')\n",
    "second_ax.plot(hist['val_precision'], label='val_precision', linestyle='--', c='orange')\n",
    "second_ax.plot(hist['recall'], label='train_recall', c='r')\n",
    "second_ax.plot(hist['val_recall'], label='val_recall', linestyle='--', c='r')\n",
    "top_ax.set_ylabel('loss')\n",
    "second_ax.set_ylabel('Precision/Recall')\n",
    "# Merge top ax legend entries\n",
    "handles, labels = top_ax.get_legend_handles_labels()\n",
    "handles2, labels2 = second_ax.get_legend_handles_labels()\n",
    "top_ax.legend(handles + handles2, labels + labels2)\n",
    "\n",
    "\n",
    "bottom_ax.plot(hist['lr'], label='lr', c='g')\n",
    "bottom_ax.set_xlabel('Epoch')\n",
    "bottom_ax.set_ylabel('Learning rate')\n",
    "\n",
    "# New plot with auc and accuracy\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(hist['auc'], label='train_auc', c='orange')\n",
    "ax.plot(hist['val_auc'], label='val_auc', linestyle='--', c='orange')\n",
    "twinax = ax.twinx()\n",
    "twinax.plot(hist['binary_accuracy'], label='train_accuracy', c='r')\n",
    "twinax.plot(hist['val_binary_accuracy'], label='val_accuracy', linestyle='--', c='r')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('AUC')\n",
    "twinax.set_ylabel('Accuracy')\n",
    "# Merge legend entries\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "twinhandles, twinlabels = twinax.get_legend_handles_labels()\n",
    "ax.legend(handles + twinhandles, labels + twinlabels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test set\n",
    "tabnet.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabnet",
   "language": "python",
   "name": "tabnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
