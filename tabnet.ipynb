{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# from tensorflow.python.framework.ops import disable_eager_execution \n",
    "# disable_eager_execution()\n",
    "# from tensorflow.python.framework.ops import enable_eager_execution\n",
    "# enable_eager_execution()\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import common tensorflow layers and activations\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Layer\n",
    "from tensorflow.keras.layers import Lambda, Multiply, Add, Rescaling\n",
    "from tensorflow.keras.activations import relu, sigmoid, softmax\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparsemax activation function implemented with tensorflow ops\n",
    "# https://arxiv.org/pdf/1602.02068.pdf\n",
    "# Copied from https://github.com/tensorflow/addons/blob/b2dafcfa74c5de268b8a5c53813bc0b89cadf386/tensorflow_addons/activations/sparsemax.py#L96\n",
    "\n",
    "def _compute_2d_sparsemax(logits, axis=-1):\n",
    "    \"\"\"Performs the sparsemax operation when axis=-1.\"\"\"\n",
    "    if axis != -1:\n",
    "        raise ValueError(\"Only axis=-1 is supported.\")\n",
    "\n",
    "    shape_op = tf.shape(logits)\n",
    "    obs = tf.math.reduce_prod(shape_op[:-1])\n",
    "    dims = shape_op[-1]\n",
    "\n",
    "    # In the paper, they call the logits z.\n",
    "    # The mean(logits) can be substracted from logits to make the algorithm\n",
    "    # more numerically stable. the instability in this algorithm comes mostly\n",
    "    # from the z_cumsum. Substacting the mean will cause z_cumsum to be close\n",
    "    # to zero. However, in practise the numerical instability issues are very\n",
    "    # minor and substacting the mean causes extra issues with inf and nan\n",
    "    # input.\n",
    "    # Reshape to [obs, dims] as it is almost free and means the remanining\n",
    "    # code doesn't need to worry about the rank.\n",
    "    z = tf.reshape(logits, [obs, dims])\n",
    "\n",
    "    # sort z\n",
    "    z_sorted, _ = tf.nn.top_k(z, k=dims)\n",
    "\n",
    "    # calculate k(z)\n",
    "    z_cumsum = tf.math.cumsum(z_sorted, axis=-1)\n",
    "    k = tf.range(1, tf.cast(dims, logits.dtype) + 1, dtype=logits.dtype)\n",
    "    z_check = 1 + k * z_sorted > z_cumsum\n",
    "    # because the z_check vector is always [1,1,...1,0,0,...0] finding the\n",
    "    # (index + 1) of the last `1` is the same as just summing the number of 1.\n",
    "    k_z = tf.math.reduce_sum(tf.cast(z_check, tf.int32), axis=-1)\n",
    "\n",
    "    # calculate tau(z)\n",
    "    # If there are inf values or all values are -inf, the k_z will be zero,\n",
    "    # this is mathematically invalid and will also cause the gather_nd to fail.\n",
    "    # Prevent this issue for now by setting k_z = 1 if k_z = 0, this is then\n",
    "    # fixed later (see p_safe) by returning p = nan. This results in the same\n",
    "    # behavior as softmax.\n",
    "    k_z_safe = tf.math.maximum(k_z, 1)\n",
    "    indices = tf.stack([tf.range(0, obs), tf.reshape(k_z_safe, [-1]) - 1], axis=1)\n",
    "    tau_sum = tf.gather_nd(z_cumsum, indices)\n",
    "    tau_z = (tau_sum - 1) / tf.cast(k_z, logits.dtype)\n",
    "\n",
    "    # calculate p\n",
    "    p = tf.math.maximum(tf.cast(0, logits.dtype), z - tf.expand_dims(tau_z, -1))\n",
    "    # If k_z = 0 or if z = nan, then the input is invalid\n",
    "    p_safe = tf.where(\n",
    "        tf.expand_dims(\n",
    "            tf.math.logical_or(tf.math.equal(k_z, 0), tf.math.is_nan(z_cumsum[:, -1])),\n",
    "            axis=-1,\n",
    "        ),\n",
    "        tf.fill([obs, dims], tf.cast(float(\"nan\"), logits.dtype)),\n",
    "        p,\n",
    "    )\n",
    "\n",
    "    # Reshape back to original size\n",
    "    p_safe = tf.reshape(p_safe, shape_op)\n",
    "    return p_safe\n",
    "\n",
    "def sparsemax(logits, axis=-1):\n",
    "    return Lambda(lambda x: _compute_2d_sparsemax(x))(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-27 01:29:22.646161: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-04-27 01:29:22.646237: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (al3615): /proc/driver/nvidia/version does not exist\n",
      "2023-04-27 01:29:22.647448: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(5, 10), dtype=float32, numpy=\n",
       " array([[3., 3., 3., 3., 4., 2., 3., 0., 3., 2.],\n",
       "        [3., 3., 2., 3., 2., 2., 3., 2., 3., 1.],\n",
       "        [3., 1., 2., 3., 3., 2., 3., 3., 3., 4.],\n",
       "        [3., 3., 3., 4., 0., 3., 3., 2., 3., 2.],\n",
       "        [5., 1., 3., 2., 1., 3., 3., 3., 2., 1.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 10), dtype=float32, numpy=\n",
       " array([[0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.20000005, 0.20000005, 0.        , 0.20000005, 0.        ,\n",
       "         0.        , 0.20000005, 0.        , 0.20000005, 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 1.        ],\n",
       "        [0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ]],\n",
       "       dtype=float32)>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.stateless_binomial(shape=(5, 10), seed=(1, 2), counts=5, probs=0.5)\n",
    "x = tf.cast(x, \"float32\")\n",
    "\n",
    "x, sparsemax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLULayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(GLULayer, self).__init__()\n",
    "        self.forward = None\n",
    "        self.units = units*2\n",
    "        self.dense = Dense(self.units)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense(inputs)\n",
    "        x1, x2 = tf.split(x, 2, axis=-1)\n",
    "        return x1 * sigmoid(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
       "array([[-0.5742565 , -0.01681935,  0.4601387 ,  0.4554238 , -0.28753227,\n",
       "        -0.77384645,  0.34789956, -0.01302431,  0.43334296,  0.80034184]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Glu Layer on normal data\n",
    "glu = GLULayer(10)\n",
    "x = tf.random.normal((1, 7))\n",
    "y = glu(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedFeatureLayer(Layer):\n",
    "    def __init__(self, units, depth=2, dense_activation=\"relu\", *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.depth = depth\n",
    "        self.dense_layers = [Dense(units, activation=dense_activation) for _ in range(depth)]\n",
    "        self.bn_layers = [BatchNormalization() for _ in range(depth)]\n",
    "        self.glu_layers = [GLULayer(units) for _ in range(depth)]\n",
    "        self.scaling = Rescaling(0.5**0.5)\n",
    "    \n",
    "    def call(self, inputs, *args, **kwargs):\n",
    "        x = inputs\n",
    "        for i in range(self.depth):\n",
    "            y = self.dense_layers[i](x)\n",
    "            y = self.bn_layers[i](y)\n",
    "            y = self.glu_layers[i](y)\n",
    "            # Skip first residual connection as input is not guaranteed to be same shape as num units\n",
    "            if i > 0:\n",
    "                x = self.scaling(x + y)\n",
    "            else:\n",
    "                x = y\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
       "array([[-0.18611856, -0.293951  , -0.18016846, -0.11435528,  0.26310158,\n",
       "        -0.0538856 ,  0.0079077 , -0.1256406 ,  0.05508604,  0.11082838]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Shared Feature Layer on normal data\n",
    "sfl = SharedFeatureLayer(units=10)\n",
    "x = tf.random.normal((1, 7))\n",
    "y = sfl(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureTransformer(Layer):\n",
    "    def __init__(self, units, shared_layer, dense_activation=\"relu\", depth=2, *args, **kwargs):\n",
    "        super().__init__( *args, **kwargs )\n",
    "        self.shared_layer = shared_layer\n",
    "        self.depth = depth\n",
    "        self.dense_layers = [Dense(units, activation=dense_activation) for _ in range(depth)]\n",
    "        self.bn_layers = [BatchNormalization() for _ in range(depth)]\n",
    "        self.glu_layers = [GLULayer(units) for _ in range(depth)]\n",
    "        self.scaling = Rescaling(0.5**0.5)\n",
    "\n",
    "    def call(self, data):\n",
    "        x = self.shared_layer(data)\n",
    "        for i in range(self.depth):\n",
    "            y = self.dense_layers[i](x)\n",
    "            y = self.bn_layers[i](y)\n",
    "            y = self.glu_layers[i](y)\n",
    "            x = self.scaling(x + y)\n",
    "\n",
    "        return x # Feature Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
       "array([[-0.00897278, -0.02062722, -0.0336582 ,  0.00802511, -0.1293786 ,\n",
       "         0.0299945 ,  0.01338341,  0.13503125, -0.0257977 ,  0.08627342]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test Feature Transformer on normal data\n",
    "sfl = SharedFeatureLayer(units=10)\n",
    "ft = FeatureTransformer(units=10, shared_layer=sfl)\n",
    "x = tf.random.normal((1, 7))\n",
    "y = ft(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentiveTransformer(Layer):\n",
    "    def __init__(self, units, gamma=1.3, dense_activation=\"relu\", *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.units = units\n",
    "        self.gamma = tf.constant(gamma, dtype=tf.float32)\n",
    "        self.activation = dense_activation\n",
    "        self.built = False\n",
    "        self.dense = Dense(self.units, activation=dense_activation)\n",
    "        self.bn = BatchNormalization()\n",
    "    \n",
    "    def call(self, data):\n",
    "        x = self.dense(data)\n",
    "        x = self.bn(x)\n",
    "        return x # Attentive\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 1), dtype=float32, numpy=\n",
       "array([[-0.0013555 ],\n",
       "       [-0.00888194],\n",
       "       [-0.02376619],\n",
       "       [-0.03610206],\n",
       "       [-0.05496325],\n",
       "       [-0.01269026]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfl = SharedFeatureLayer(units=10)\n",
    "model = Sequential([\n",
    "    Input(shape=(7,)),\n",
    "    FeatureTransformer(units=10, shared_layer=sfl),\n",
    "    AttentiveTransformer(units=10),\n",
    "    Dense(1)\n",
    "])\n",
    "model(tf.random.normal((6, 7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNet(Model):\n",
    "    def __init__(self, dim_features, dim_attention, dim_output, sparsity=0.0001, num_steps=5, gamma=1.5, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_step = num_steps\n",
    "        self.dim_features = dim_features\n",
    "        self.dim_attention = dim_attention # Currently keep it simple and just have equal attention and pre_output dimensions\n",
    "        self.dim_pre_output = dim_attention\n",
    "        self.dim_output = dim_output\n",
    "        self.gamma = tf.constant(gamma)\n",
    "        self.eps = tf.constant(1e-5)\n",
    "        self.sparsity_coef = sparsity\n",
    "        self.shared_layer = SharedFeatureLayer(units=self.dim_attention+self.dim_pre_output, depth=2, name=\"shared_feature_layer\")\n",
    "        self.feature_transformers = [FeatureTransformer(units=self.dim_attention+self.dim_pre_output, shared_layer=self.shared_layer, name=f\"feat_{i}\") for i in range(num_steps + 1)]\n",
    "        self.attention_layers = [AttentiveTransformer(units=self.dim_features, name=f\"attn_{i+1}\") for i in range(num_steps)]\n",
    "        self.norm_in = BatchNormalization(name=\"norm_in\")\n",
    "        self.output_dense = Dense(dim_output, name=\"output\")\n",
    "        self.attn_activation = _compute_2d_sparsemax\n",
    "    \n",
    "    def call(self, data):\n",
    "        normed_data = self.norm_in(data)\n",
    "\n",
    "        d0, a_i = tf.split(self.feature_transformers[0](normed_data), 2, axis=-1)\n",
    "        decision = tf.zeros_like(d0)\n",
    "        priors = []\n",
    "        entropy = 0.\n",
    "        for i in range(self.num_step):\n",
    "            candidate_mask = self.attention_layers[i](a_i)\n",
    "            for prior in priors:\n",
    "                candidate_mask = candidate_mask*(self.gamma - prior)\n",
    "            candidate_mask = self.attn_activation(candidate_mask)\n",
    "            decision_entropy = -tf.reduce_mean(tf.reduce_sum(candidate_mask * tf.math.log(candidate_mask + self.eps), axis=-1))\n",
    "            entropy += decision_entropy\n",
    "\n",
    "            # Why does self.add_loss not work here?\n",
    "            \n",
    "            \n",
    "            normed_features = normed_data * candidate_mask\n",
    "            x = self.feature_transformers[i+1](normed_features)\n",
    "            d_i, a_i = tf.split(x, 2, axis=-1)\n",
    "            decision += relu(d_i)\n",
    "        \n",
    "        return self.output_dense(decision), entropy\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred, entropy = self(x, training=True)\n",
    "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "            loss += self.sparsity_coef * entropy\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# Load telematics data\n",
    "telematics = pd.read_csv(\"./telematics_syn-032021.csv\")\n",
    "response = 'AMT_Claim'\n",
    "outputs = ['NB_Claim']\n",
    "# outputs = telematics.pop([\"NB_Claim\",\"AMT_Claim\"])\n",
    "# response = outputs['AMT_Claim']\n",
    "\n",
    "# Split data into train and test\n",
    "data_train, data_test = train_test_split(telematics, test_size=0.3, random_state=42)\n",
    "data_val, data_test = train_test_split(data_test, test_size=0.33, random_state=42)\n",
    "\n",
    "# Mean target encode categorical variables\n",
    "aggregate_fn = np.mean\n",
    "for col in ['Marital', 'Insured.sex', 'Car.use', 'Region', 'Territory']:\n",
    "    encoding = data_train.groupby(col)[response].aggregate(aggregate_fn)\n",
    "    data_train[col] = data_train[col].map(encoding)\n",
    "    data_val[col] = data_val[col].map(encoding)\n",
    "    data_test[col] = data_test[col].map(encoding)\n",
    "\n",
    "# Split into X and y\n",
    "X_train = data_train.drop(outputs, axis=1)\n",
    "y_train = data_train[response]\n",
    "X_val = data_val.drop(outputs, axis=1)\n",
    "y_val = data_val[response]\n",
    "X_test = data_test.drop(outputs, axis=1)\n",
    "y_test = data_test[response]\n",
    "\n",
    "# Make tensorflow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train.values, y_train.values))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=4048).batch(BATCH_SIZE)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val.values, y_val.values))\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test.values, y_test.values))\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabnet = TabNet(dim_features=X_train.shape[1], dim_attention=64, dim_output=1)\n",
    "\n",
    "tabnet.compile( \n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), \n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    ")\n",
    "tabnet.build(X_train.sample(1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(7, 1), dtype=float32, numpy=\n",
       " array([[ 0.00022651],\n",
       "        [ 0.00384692],\n",
       "        [-0.00440566],\n",
       "        [-0.00742131],\n",
       "        [-0.00271641],\n",
       "        [-0.00949951],\n",
       "        [-0.00217473]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=18.18792>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabnet(tf.random.normal((7, X_train.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tab_net_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "shared_feature_layer (Shared multiple                  90240     \n",
      "_________________________________________________________________\n",
      "feat_0 (FeatureTransformer)  multiple                  190336    \n",
      "_________________________________________________________________\n",
      "feat_1 (FeatureTransformer)  multiple                  190336    \n",
      "_________________________________________________________________\n",
      "feat_2 (FeatureTransformer)  multiple                  190336    \n",
      "_________________________________________________________________\n",
      "feat_3 (FeatureTransformer)  multiple                  190336    \n",
      "_________________________________________________________________\n",
      "feat_4 (FeatureTransformer)  multiple                  190336    \n",
      "_________________________________________________________________\n",
      "feat_5 (FeatureTransformer)  multiple                  190336    \n",
      "_________________________________________________________________\n",
      "attn_1 (AttentiveTransformer multiple                  3519      \n",
      "_________________________________________________________________\n",
      "attn_2 (AttentiveTransformer multiple                  3519      \n",
      "_________________________________________________________________\n",
      "attn_3 (AttentiveTransformer multiple                  3519      \n",
      "_________________________________________________________________\n",
      "attn_4 (AttentiveTransformer multiple                  3519      \n",
      "_________________________________________________________________\n",
      "attn_5 (AttentiveTransformer multiple                  3519      \n",
      "_________________________________________________________________\n",
      "norm_in (BatchNormalization) multiple                  204       \n",
      "_________________________________________________________________\n",
      "output (Dense)               multiple                  65        \n",
      "=================================================================\n",
      "Total params: 708,680\n",
      "Trainable params: 704,484\n",
      "Non-trainable params: 4,196\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tabnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-27 01:34:20.585217: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1334/2188 [=================>............] - ETA: 14s - loss: 1600541.7500 - root_mean_squared_error: 1265.1252"
     ]
    }
   ],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "history = tabnet.fit(train_dataset, epochs=10, validation_data=val_dataset, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot history for training and validation set\n",
    "plt.plot(history.history['root_mean_squared_error'], label='train')\n",
    "plt.plot(history.history['val_root_mean_squared_error'], label='validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test set\n",
    "tabnet.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabnet",
   "language": "python",
   "name": "tabnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
