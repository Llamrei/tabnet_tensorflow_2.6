{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import common tensorflow layers and activations\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Layer\n",
    "from tensorflow.keras.layers import Lambda, Multiply, Add \n",
    "from tensorflow.keras.activations import relu, sigmoid, softmax\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLULayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(GLULayer, self).__init__()\n",
    "        self.forward = None\n",
    "        self.units = units*2\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = Dense(self.units)(inputs)\n",
    "        x1, x2 = tf.split(x, 2, axis=-1)\n",
    "        return x1 * sigmoid(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-25 18:48:43.475519: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-04-25 18:48:43.475615: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (al3615): /proc/driver/nvidia/version does not exist\n",
      "2023-04-25 18:48:43.476388: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
       "array([[ 0.23215452,  0.9407819 ,  0.04402405,  0.07932476,  0.28471863,\n",
       "        -0.23389688, -0.30013853,  0.10115302, -0.8447979 ,  0.5125168 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Glu Layer on normal data\n",
    "glu = GLULayer(10)\n",
    "x = tf.random.normal((1, 7))\n",
    "y = glu(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedFeatureLayer(Layer):\n",
    "    def __init__(self, units, depth=2, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.units = units\n",
    "        self.depth = depth\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "        _in = Input(shape=input_shape[1:])\n",
    "        x = _in\n",
    "        for i in range(self.depth):\n",
    "            y = Dense(self.units)(x)\n",
    "            y = BatchNormalization()(y)\n",
    "            y = GLULayer(self.units)(y)\n",
    "            # Skip first residual connection as input is not guaranteed to be same shape as num units\n",
    "            if i > 0:\n",
    "                x = (x + y)*0.5**0.5\n",
    "            else:\n",
    "                x = y\n",
    "        self.forward = Model(_in, y)\n",
    "    \n",
    "    def call(self, inputs, *args, **kwargs):\n",
    "        return self.forward(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
       "array([[ 0.15125495, -0.09653445, -0.18121697, -0.10523367,  0.09352842,\n",
       "         0.00124904,  0.06591543, -0.03764122, -0.36132282, -0.12428884]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Shared Feature Layer on normal data\n",
    "sfl = SharedFeatureLayer(units=10)\n",
    "x = tf.random.normal((1, 7))\n",
    "y = sfl(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureTransformer(Layer):\n",
    "    def __init__(self, units, shared_layer, depth=2, *args, **kwargs):\n",
    "        super().__init__( *args, **kwargs )\n",
    "        self.units = units\n",
    "        self.shared_layer = shared_layer\n",
    "        self.depth = depth\n",
    "        self.forward = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "        # self.shared_layer.build(input_shape)\n",
    "        _in = Input(shape=input_shape[1:])\n",
    "        shared_output = self.shared_layer(_in)\n",
    "        x = shared_output\n",
    "        for _ in range(self.depth):\n",
    "            y = Dense(self.units)(x)\n",
    "            y = BatchNormalization()(y)\n",
    "            y = GLULayer(self.units)(y)\n",
    "            x = (x + y)*0.5**0.5\n",
    "        self.forward = Model(_in, x)\n",
    "    \n",
    "    def call(self, data):\n",
    "        return self.forward(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
       "array([[ 0.01654652, -0.02763228,  0.09033699, -0.08615315, -0.00484035,\n",
       "        -0.07045442,  0.07713187, -0.05966711,  0.00776217, -0.07103664]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test Feature Transformer on normal data\n",
    "sfl = SharedFeatureLayer(units=10)\n",
    "ft = FeatureTransformer(units=10, shared_layer=sfl)\n",
    "x = tf.random.normal((1, 7))\n",
    "y = ft(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentiveTransformer(Model):\n",
    "    def __init__(self, units, gamma=1.3, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.units = units\n",
    "        self.gamma = tf.constant(gamma, dtype=tf.float32)\n",
    "        self.forward = None\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # super().build(input_shape)\n",
    "        _in = Input(shape=input_shape[1:], name=f\"{self._name}_input\")\n",
    "        x = Dense(self.units, name=f\"{self._name}_dense\")(_in)\n",
    "        x = BatchNormalization(name=f\"{self._name}_BN\")(x)\n",
    "        x = softmax(x, axis=-1)\n",
    "        self.forward = Model(_in, x)\n",
    "    \n",
    "    def call(self, data):\n",
    "        return self.forward(data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNet(Model):\n",
    "    def __init__(self, dim_features, dim_attention, dim_output, num_steps=3, gamma=1.3, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_step = num_steps\n",
    "        self.dim_features = dim_features\n",
    "        self.dim_attention = dim_attention # Currently keep it simple and just have equal attention and pre_output dimensions\n",
    "        self.dim_pre_output = dim_attention\n",
    "        self.dim_output = dim_output\n",
    "        self.gamma = tf.constant(gamma)\n",
    "\n",
    "        self.shared_layer = SharedFeatureLayer(units=dim_attention*2, depth=2, name=\"shared_feature_layer\")\n",
    "        self.feature_transformers = [FeatureTransformer(units=dim_attention*2, shared_layer=self.shared_layer, name=\"init_feature_transformer\")]\n",
    "        self.attention_layers = []\n",
    "\n",
    "        # Define model forward pass\n",
    "\n",
    "        _in = Input(shape=(self.dim_features,), name=\"tabnet_input\")\n",
    "        norm_in = BatchNormalization(name=\"norm_in\")(_in)\n",
    "        \n",
    "\n",
    "        d0, a_i = tf.split(self.feature_transformers[0](norm_in), num_or_size_splits=2, axis=-1)\n",
    "        priors = []\n",
    "        d = tf.zeros_like(d0)\n",
    "        for i in range(num_steps):\n",
    "            new_attention_layer = AttentiveTransformer(units=self.dim_features, name=f\"attention_layer_{i+1}\")\n",
    "            new_feature_layer = FeatureTransformer(units=dim_attention*2, shared_layer=self.shared_layer, name=f\"feature_transformer_{i+1}\")\n",
    "\n",
    "            candidate_mask = new_attention_layer(a_i)\n",
    "            for prior in priors:\n",
    "                candidate_mask *= (self.gamma-prior)\n",
    "            priors.append(candidate_mask)\n",
    "\n",
    "            masked_input = tf.math.multiply(norm_in, candidate_mask)\n",
    "\n",
    "            new_features = new_feature_layer(masked_input)\n",
    "            d_i, a_i = tf.split(new_features, num_or_size_splits=2, axis=-1)\n",
    "            d = d + relu(d_i)\n",
    "\n",
    "            self.attention_layers.append(new_attention_layer)\n",
    "            self.feature_transformers.append(new_feature_layer)\n",
    "        _out = Dense(dim_output)(d)\n",
    "        self.forward = Model(_in, _out)\n",
    "    \n",
    "    def call(self, data):\n",
    "        return self.forward(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
       "array([[ 0.00054083],\n",
       "       [-0.01297351],\n",
       "       [-0.0210685 ],\n",
       "       [-0.01517237],\n",
       "       [-0.00419401],\n",
       "       [-0.04388188],\n",
       "       [-0.02505299],\n",
       "       [-0.05104399],\n",
       "       [-0.02460882],\n",
       "       [-0.08395625]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test TabNet on normal data\n",
    "tabnet = TabNet(dim_features=7, dim_attention=10, dim_output=1)\n",
    "x = tf.random.normal((10, 7))\n",
    "y = tabnet(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGgAAAA8CAIAAAAFYWgXAAAABmJLR0QA/wD/AP+gvaeTAAAFdUlEQVR4nO2bXUhTbxzHf5vh3qwpOgXdiCjEnKFEkqVDQTH0ohAmCiK+jDS8KDRR1CDzxvAlCdOL0i6Kwncvu4kc5csMpvNImQqKbnOiWA5XKpvndHFiTKez/f5q2f/5XJ3n2Xl+57sPZw/PnrNxGIYBgvtw/3SA4woRh4SIQ0LEITnh2BgaGnr06NGfivKXU1RUdOXKFXtz2x2n1+u7urqOPNIxoKurS6/XO/accD6ps7PzqPIcGzgczo4eMschIeKQEHFIiDgkRBwSIg4JEYeEiENCxCEh4pAQcUiIOCREHBIiDslhifv48ePNmzfPnDlzSPX/OEhxT548WV1d3etVmqZTUlJaW1vX1tawwQ4yjyO9vb1yudzLyysqKmpwcBB9RYy479+/NzQ0uCrK5RqNxuTkZGyqA85jR61WT01NaTSa4eFhmqavXbu2tLSEuyhGXEFBwczMzL6n+fv7I4oj+M08ADA2NlZaWnry5Em5XH7//n2LxTIxMYG7qNvi8vLyXrx4AQA+Pj4cDsdisczPz6empkokEpFIFB0dPTo6+qs0lwsAOp0uNjZWIBBERET09/e7Lj45OVlSUhIQEGAyme7du+fn5xcYGOi4ld/R0REeHs7j8YKDg7u7u3fN46L+rVu37McURZ0+ffry5cvuGvgF40B7e/uOnl2prq4GgG/fvrHNS5cuJSQkmEymqakpqVQaFRXF9qtUKj6f//DhQ5PJpNPp5HK5SCQyGAwuKsfExHh4eADA7du3R0ZGVldXFQqFt7e31WplGOb58+cKhWJ6enp5efnGjRtcLler1Trn2ZeZmZny8nKpVEpR1G8OAYD29vZtPY4NnLiLFy82NTWxx1lZWRKJhD1WqVS+vr72UTqdjsPh3Llzx3Xxu3fvAoBer2eb9fX1AGAwGKxWq7+//8TEBNs/NzcHAOnp6c55XGN/WMXj8YqLi2022++Mcha3y1Mud9FqtQAwPDzc2NjY3d3N5/N3PS08PFwmk42Njbmuxs6M9iJCoRAArFYrRVFLS0vnz593PPnTp0/uppVKpevr6xqNpqqqqq6uTigUPnjwwN0icCDrOKPRmJSUlJ+fn5SUlJaWxuz98yeJRLLvooGdGZ1ZXl4GgB2fdIqiEIH5fH5cXNybN29CQ0Pb2toQFeC/i7NYLAqFQiwWa7XajIyMvW43loWFhXPnzuEuJBaLAaCjowM33Bkej6dUKm02G244Rhz7dJa9sz58+DA7O5ubm8tO6vZ+ZzQajclkyszMdF18Y2MDALa2ttgmTdMAYLPZIiIixGJxWVlZTU3NwsKC2Wx+//59cXHxjjxuMTs7i15sYsT5+PgAgFarbWlpYeO+evXKbDZ3dnYODAz8+PHj8+fParU6MDDw69evFRUVi4uL4+PjOTk5KpXq+vXrLipvbm6yS5a+vj6GYaxWq1qtBoB3797xeLzKysrNzc3S0tKgoCBvb+/4+Hi2mmMeo9G4V3GDwRAQEJCdnT09PW02mx8/fkxRVFVVFcIAAGo5srKycvXqVYlE8vr1a4ZhSkpKTp06FRYW1tPT8/LlS5FIVFhYuLW1RdP0s2fPQkNDPT09Q0JCmpubaZp2XTk2NtYerLa21nGR1djYyDDM06dPz549KxAIIiMj3759u2uevbBYLImJiUKhUCAQXLhwobKycm1tbd83ywIHshz5H+IsjmwrISHikBy1uC9fvnD2RqlU/rXFd3AA3xzcIiQkhDm0PwgcavEdkI8qEiIOCRGHhIhDQsQhIeKQEHFIiDgkRBwSIg4JEYeEiENCxCHZZXckNTX16HMcO7bdcTKZ7GA3rf4ZlEqlTCZz7OEc2QbWPwaZ45AQcUiIOCREHJKfmltzeToxg1oAAAAASUVORK5CYII=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(tabnet, expand_nested=True, show_layer_names=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabnet",
   "language": "python",
   "name": "tabnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
