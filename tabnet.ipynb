{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# from tensorflow.python.framework.ops import disable_eager_execution \n",
    "# disable_eager_execution()\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import common tensorflow layers and activations\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Layer\n",
    "from tensorflow.keras.layers import Lambda, Multiply, Add, Rescaling \n",
    "from tensorflow.keras.activations import relu, sigmoid, softmax\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparsemax activation function implemented with tensorflow ops\n",
    "# https://arxiv.org/pdf/1602.02068.pdf\n",
    "# Copied from https://github.com/tensorflow/addons/blob/b2dafcfa74c5de268b8a5c53813bc0b89cadf386/tensorflow_addons/activations/sparsemax.py#L96\n",
    "\n",
    "def _compute_2d_sparsemax(logits, axis=-1):\n",
    "    \"\"\"Performs the sparsemax operation when axis=-1.\"\"\"\n",
    "    if axis != -1:\n",
    "        raise ValueError(\"Only axis=-1 is supported.\")\n",
    "\n",
    "    shape_op = tf.shape(logits)\n",
    "    obs = tf.math.reduce_prod(shape_op[:-1])\n",
    "    dims = shape_op[-1]\n",
    "\n",
    "    # In the paper, they call the logits z.\n",
    "    # The mean(logits) can be substracted from logits to make the algorithm\n",
    "    # more numerically stable. the instability in this algorithm comes mostly\n",
    "    # from the z_cumsum. Substacting the mean will cause z_cumsum to be close\n",
    "    # to zero. However, in practise the numerical instability issues are very\n",
    "    # minor and substacting the mean causes extra issues with inf and nan\n",
    "    # input.\n",
    "    # Reshape to [obs, dims] as it is almost free and means the remanining\n",
    "    # code doesn't need to worry about the rank.\n",
    "    z = tf.reshape(logits, [obs, dims])\n",
    "\n",
    "    # sort z\n",
    "    z_sorted, _ = tf.nn.top_k(z, k=dims)\n",
    "\n",
    "    # calculate k(z)\n",
    "    z_cumsum = tf.math.cumsum(z_sorted, axis=-1)\n",
    "    k = tf.range(1, tf.cast(dims, logits.dtype) + 1, dtype=logits.dtype)\n",
    "    z_check = 1 + k * z_sorted > z_cumsum\n",
    "    # because the z_check vector is always [1,1,...1,0,0,...0] finding the\n",
    "    # (index + 1) of the last `1` is the same as just summing the number of 1.\n",
    "    k_z = tf.math.reduce_sum(tf.cast(z_check, tf.int32), axis=-1)\n",
    "\n",
    "    # calculate tau(z)\n",
    "    # If there are inf values or all values are -inf, the k_z will be zero,\n",
    "    # this is mathematically invalid and will also cause the gather_nd to fail.\n",
    "    # Prevent this issue for now by setting k_z = 1 if k_z = 0, this is then\n",
    "    # fixed later (see p_safe) by returning p = nan. This results in the same\n",
    "    # behavior as softmax.\n",
    "    k_z_safe = tf.math.maximum(k_z, 1)\n",
    "    indices = tf.stack([tf.range(0, obs), tf.reshape(k_z_safe, [-1]) - 1], axis=1)\n",
    "    tau_sum = tf.gather_nd(z_cumsum, indices)\n",
    "    tau_z = (tau_sum - 1) / tf.cast(k_z, logits.dtype)\n",
    "\n",
    "    # calculate p\n",
    "    p = tf.math.maximum(tf.cast(0, logits.dtype), z - tf.expand_dims(tau_z, -1))\n",
    "    # If k_z = 0 or if z = nan, then the input is invalid\n",
    "    p_safe = tf.where(\n",
    "        tf.expand_dims(\n",
    "            tf.math.logical_or(tf.math.equal(k_z, 0), tf.math.is_nan(z_cumsum[:, -1])),\n",
    "            axis=-1,\n",
    "        ),\n",
    "        tf.fill([obs, dims], tf.cast(float(\"nan\"), logits.dtype)),\n",
    "        p,\n",
    "    )\n",
    "\n",
    "    # Reshape back to original size\n",
    "    p_safe = tf.reshape(p_safe, shape_op)\n",
    "    return p_safe\n",
    "\n",
    "def sparsemax(logits, axis=-1):\n",
    "    \n",
    "    return Lambda(lambda x: _compute_2d_sparsemax(x))(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 23:57:35.074139: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-04-26 23:57:35.074169: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (al3615): /proc/driver/nvidia/version does not exist\n",
      "2023-04-26 23:57:35.075544: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(5, 10), dtype=float32, numpy=\n",
       " array([[3., 3., 3., 3., 4., 2., 3., 0., 3., 2.],\n",
       "        [3., 3., 2., 3., 2., 2., 3., 2., 3., 1.],\n",
       "        [3., 1., 2., 3., 3., 2., 3., 3., 3., 4.],\n",
       "        [3., 3., 3., 4., 0., 3., 3., 2., 3., 2.],\n",
       "        [5., 1., 3., 2., 1., 3., 3., 3., 2., 1.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 10), dtype=float32, numpy=\n",
       " array([[0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.20000005, 0.20000005, 0.        , 0.20000005, 0.        ,\n",
       "         0.        , 0.20000005, 0.        , 0.20000005, 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 1.        ],\n",
       "        [0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ]],\n",
       "       dtype=float32)>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.stateless_binomial(shape=(5, 10), seed=(1, 2), counts=5, probs=0.5)\n",
    "x = tf.cast(x, \"float32\")\n",
    "\n",
    "x, sparsemax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLULayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(GLULayer, self).__init__()\n",
    "        self.forward = None\n",
    "        self.units = units*2\n",
    "        self.dense = Dense(self.units)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense(inputs)\n",
    "        x1, x2 = tf.split(x, 2, axis=-1)\n",
    "        return x1 * sigmoid(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
       "array([[-0.96361774,  0.6793994 ,  0.1317401 ,  0.04591553, -0.35321206,\n",
       "        -0.21881306,  0.15043025,  0.13077098, -0.03897366,  0.29308307]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Glu Layer on normal data\n",
    "glu = GLULayer(10)\n",
    "x = tf.random.normal((1, 7))\n",
    "y = glu(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedFeatureLayer(Layer):\n",
    "    def __init__(self, units, depth=2, dense_activation=\"relu\", *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.depth = depth\n",
    "        self.dense_layers = [Dense(units, activation=dense_activation) for _ in range(depth)]\n",
    "        self.bn_layers = [BatchNormalization() for _ in range(depth)]\n",
    "        self.glu_layers = [GLULayer(units) for _ in range(depth)]\n",
    "        self.scaling = Rescaling(0.5**0.5)\n",
    "    \n",
    "    def call(self, inputs, *args, **kwargs):\n",
    "        x = inputs\n",
    "        for i in range(self.depth):\n",
    "            y = self.dense_layers[i](x)\n",
    "            y = self.bn_layers[i](y)\n",
    "            y = self.glu_layers[i](y)\n",
    "            # Skip first residual connection as input is not guaranteed to be same shape as num units\n",
    "            if i > 0:\n",
    "                x = self.scaling(x + y)\n",
    "            else:\n",
    "                x = y\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
       "array([[ 0.02939319,  0.44278663, -0.6480196 ,  0.09077926,  0.07966833,\n",
       "         0.15026082, -0.13508579,  0.09931763, -0.06332603, -0.20511775]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Shared Feature Layer on normal data\n",
    "sfl = SharedFeatureLayer(units=10)\n",
    "x = tf.random.normal((1, 7))\n",
    "y = sfl(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureTransformer(Layer):\n",
    "    def __init__(self, units, shared_layer, dense_activation=\"relu\", depth=2, *args, **kwargs):\n",
    "        super().__init__( *args, **kwargs )\n",
    "        self.shared_layer = shared_layer\n",
    "        self.depth = depth\n",
    "        self.dense_layers = [Dense(units, activation=dense_activation) for _ in range(depth)]\n",
    "        self.bn_layers = [BatchNormalization() for _ in range(depth)]\n",
    "        self.glu_layers = [GLULayer(units) for _ in range(depth)]\n",
    "        self.scaling = Rescaling(0.5**0.5)\n",
    "\n",
    "    def call(self, data):\n",
    "        x = self.shared_layer(data)\n",
    "        for i in range(self.depth):\n",
    "            y = self.dense_layers[i](x)\n",
    "            y = self.bn_layers[i](y)\n",
    "            y = self.glu_layers[i](y)\n",
    "            x = self.scaling(x + y)\n",
    "\n",
    "        return x # Feature Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
       "array([[-0.11730861,  0.12405972, -0.01578275,  0.04211384,  0.0752199 ,\n",
       "        -0.0251449 , -0.0819028 ,  0.04296739,  0.01050517, -0.02078931]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test Feature Transformer on normal data\n",
    "sfl = SharedFeatureLayer(units=10)\n",
    "ft = FeatureTransformer(units=10, shared_layer=sfl)\n",
    "x = tf.random.normal((1, 7))\n",
    "y = ft(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentiveTransformer(Layer):\n",
    "    def __init__(self, units, gamma=1.3, dense_activation=\"relu\", *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.units = units\n",
    "        self.gamma = tf.constant(gamma, dtype=tf.float32)\n",
    "        self.activation = dense_activation\n",
    "        self.built = False\n",
    "        self.dense = Dense(self.units, activation=dense_activation)\n",
    "        self.bn = BatchNormalization()\n",
    "    \n",
    "    def call(self, data):\n",
    "        x = self.dense(data)\n",
    "        x = self.bn(x)\n",
    "        return x # Attentive\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNet(Model):\n",
    "    def __init__(self, dim_features, dim_attention, dim_output, sparsity=0.0001, num_steps=5, gamma=1.5, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_step = num_steps\n",
    "        self.dim_features = dim_features\n",
    "        self.dim_attention = dim_attention # Currently keep it simple and just have equal attention and pre_output dimensions\n",
    "        self.dim_pre_output = dim_attention\n",
    "        self.dim_output = dim_output\n",
    "        self.gamma = tf.constant(gamma)\n",
    "        self.eps = tf.constant(1e-5)\n",
    "        self.sparsity_coef = sparsity\n",
    "        self.priors = []\n",
    "        self.shared_layer = SharedFeatureLayer(units=self.dim_attention+self.dim_pre_output, depth=2, name=\"shared_feature_layer\")\n",
    "        self.feature_transformers = [FeatureTransformer(units=self.dim_attention+self.dim_pre_output, shared_layer=self.shared_layer, name=f\"feat_{i}\") for i in range(num_steps + 1)]\n",
    "        self.attention_layers = [AttentiveTransformer(units=self.dim_features, name=f\"attn_{i+1}\") for i in range(num_steps)]\n",
    "        self.norm_in = BatchNormalization(name=\"norm_in\")\n",
    "        self.output_dense = Dense(dim_output, name=\"output\")\n",
    "        self.attn_activation = _compute_2d_sparsemax\n",
    "    \n",
    "    def call(self, data):\n",
    "        normed_data = self.norm_in(data)\n",
    "\n",
    "        d0, a_i = tf.split(self.feature_transformers[0](normed_data), 2, axis=-1)\n",
    "        decision = tf.zeros_like(d0)\n",
    "\n",
    "        for i in range(self.num_step):\n",
    "            candidate_mask = self.attention_layers[i](a_i)\n",
    "            # print(\"before_priors\", candidate_mask)\n",
    "            for prior in self.priors:\n",
    "                candidate_mask = candidate_mask*(self.gamma - prior)\n",
    "            # print(\"after_priors\", candidate_mask)\n",
    "            candidate_mask = self.attn_activation(candidate_mask)\n",
    "\n",
    "            if self.sparsity_coef > 0:\n",
    "                # print(candidate_mask)\n",
    "                decision_entropy = -tf.reduce_mean(tf.reduce_sum(candidate_mask, axis=-1))/self.num_step\n",
    "                self.add_loss(lambda : self.sparsity_coef * decision_entropy)\n",
    "\n",
    "            self.priors.append(candidate_mask)\n",
    "            \n",
    "            \n",
    "            normed_features = normed_data * candidate_mask\n",
    "            x = self.feature_transformers[i+1](normed_features)\n",
    "            d_i, a_i = tf.split(x, 2, axis=-1)\n",
    "            decision += relu(d_i)\n",
    "        \n",
    "        return self.output_dense(decision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data\n",
    "\n",
    "BATCH_SIZE = 16384\n",
    "\n",
    "# Load telematics data\n",
    "telematics = pd.read_csv(\"./telematics_syn-032021.csv\")\n",
    "response = 'AMT_Claim'\n",
    "outputs = ['NB_Claim']\n",
    "# outputs = telematics.pop([\"NB_Claim\",\"AMT_Claim\"])\n",
    "# response = outputs['AMT_Claim']\n",
    "\n",
    "# Split data into train and test\n",
    "data_train, data_test = train_test_split(telematics, test_size=0.3, random_state=42)\n",
    "data_val, data_test = train_test_split(data_test, test_size=0.33, random_state=42)\n",
    "\n",
    "# Mean target encode categorical variables\n",
    "aggregate_fn = np.mean\n",
    "for col in ['Marital', 'Insured.sex', 'Car.use', 'Region', 'Territory']:\n",
    "    encoding = data_train.groupby(col)[response].aggregate(aggregate_fn)\n",
    "    data_train[col] = data_train[col].map(encoding)\n",
    "    data_val[col] = data_val[col].map(encoding)\n",
    "    data_test[col] = data_test[col].map(encoding)\n",
    "\n",
    "# Split into X and y\n",
    "X_train = data_train.drop(outputs, axis=1)\n",
    "y_train = data_train[response]\n",
    "X_val = data_val.drop(outputs, axis=1)\n",
    "y_val = data_val[response]\n",
    "X_test = data_test.drop(outputs, axis=1)\n",
    "y_test = data_test[response]\n",
    "\n",
    "# Make tensorflow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train.values, y_train.values))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=4048).batch(BATCH_SIZE)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val.values, y_val.values))\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test.values, y_test.values))\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabnet = TabNet(dim_features=X_train.shape[1], dim_attention=64, dim_output=1)\n",
    "\n",
    "\n",
    "\n",
    "tabnet.compile( \n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), \n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    run_eagerly=False,\n",
    "    metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    ")\n",
    "dummy = X_train.sample(1).values\n",
    "tabnet.build(dummy.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tab_net_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "shared_feature_layer (Shared multiple                  90240     \n",
      "_________________________________________________________________\n",
      "feat_0 (FeatureTransformer)  multiple                  190336    \n",
      "_________________________________________________________________\n",
      "feat_1 (FeatureTransformer)  multiple                  190336    \n",
      "_________________________________________________________________\n",
      "feat_2 (FeatureTransformer)  multiple                  190336    \n",
      "_________________________________________________________________\n",
      "feat_3 (FeatureTransformer)  multiple                  190336    \n",
      "_________________________________________________________________\n",
      "feat_4 (FeatureTransformer)  multiple                  190336    \n",
      "_________________________________________________________________\n",
      "feat_5 (FeatureTransformer)  multiple                  190336    \n",
      "_________________________________________________________________\n",
      "attn_1 (AttentiveTransformer multiple                  3519      \n",
      "_________________________________________________________________\n",
      "attn_2 (AttentiveTransformer multiple                  3519      \n",
      "_________________________________________________________________\n",
      "attn_3 (AttentiveTransformer multiple                  3519      \n",
      "_________________________________________________________________\n",
      "attn_4 (AttentiveTransformer multiple                  3519      \n",
      "_________________________________________________________________\n",
      "attn_5 (AttentiveTransformer multiple                  3519      \n",
      "_________________________________________________________________\n",
      "norm_in (BatchNormalization) multiple                  204       \n",
      "_________________________________________________________________\n",
      "output (Dense)               multiple                  65        \n",
      "=================================================================\n",
      "Total params: 708,680\n",
      "Trainable params: 704,484\n",
      "Non-trainable params: 4,196\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tabnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "An op outside of the function building code is being passed\na \"Graph\" tensor. It is possible to have Graph tensors\nleak out of the function building context by including a\ntf.init_scope in your function building code.\nFor example, the following function will fail:\n  @tf.function\n  def has_init_scope():\n    my_constant = tf.constant(1.)\n    with tf.init_scope():\n      added = my_constant * 2\nThe graph tensor has name: Reshape_2:0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m early_stopping \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mEarlyStopping(patience\u001b[39m=\u001b[39m\u001b[39m6\u001b[39m, restore_best_weights\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m lr_on_plateau \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mReduceLROnPlateau(patience\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, factor\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, min_lr\u001b[39m=\u001b[39m\u001b[39m1e-5\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m history \u001b[39m=\u001b[39m tabnet\u001b[39m.\u001b[39;49mfit(train_dataset, epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49mval_dataset, callbacks\u001b[39m=\u001b[39;49m[early_stopping, lr_on_plateau])\n",
      "File \u001b[0;32m~/projects/deep_learning_vs_gbdt/venv/lib/python3.9/site-packages/keras/engine/training.py:1184\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1178\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1179\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   1180\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   1181\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1182\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   1183\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1184\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1185\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1186\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/projects/deep_learning_vs_gbdt/venv/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    882\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    884\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 885\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    887\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    888\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/projects/deep_learning_vs_gbdt/venv/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:950\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[39mpass\u001b[39;00m  \u001b[39m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[1;32m    947\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m     \u001b[39m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[1;32m    949\u001b[0m     \u001b[39m# stateless function.\u001b[39;00m\n\u001b[0;32m--> 950\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    951\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    952\u001b[0m   _, _, _, filtered_flat_args \u001b[39m=\u001b[39m \\\n\u001b[1;32m    953\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn\u001b[39m.\u001b[39m_function_spec\u001b[39m.\u001b[39mcanonicalize_function_inputs(  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    954\u001b[0m           \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/projects/deep_learning_vs_gbdt/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py:3039\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3036\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   3037\u001b[0m   (graph_function,\n\u001b[1;32m   3038\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   3040\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/projects/deep_learning_vs_gbdt/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1963\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1960\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1961\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1962\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1963\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1964\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1965\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m     args,\n\u001b[1;32m   1967\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1968\u001b[0m     executing_eagerly)\n\u001b[1;32m   1969\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/projects/deep_learning_vs_gbdt/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py:591\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    590\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 591\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    592\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    593\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    594\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    595\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    596\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    597\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    598\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    599\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    600\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    603\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    604\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/projects/deep_learning_vs_gbdt/venv/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:75\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[39mif\u001b[39;00m keras_symbolic_tensors:\n\u001b[1;32m     72\u001b[0m     \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_SymbolicException(\n\u001b[1;32m     73\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mInputs to eager execution function cannot be Keras symbolic \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtensors, but found \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(keras_symbolic_tensors))\n\u001b[0;32m---> 75\u001b[0m   \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m     76\u001b[0m \u001b[39m# pylint: enable=protected-access\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[39mreturn\u001b[39;00m tensors\n",
      "File \u001b[0;32m~/projects/deep_learning_vs_gbdt/venv/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 59\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     60\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     62\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: An op outside of the function building code is being passed\na \"Graph\" tensor. It is possible to have Graph tensors\nleak out of the function building context by including a\ntf.init_scope in your function building code.\nFor example, the following function will fail:\n  @tf.function\n  def has_init_scope():\n    my_constant = tf.constant(1.)\n    with tf.init_scope():\n      added = my_constant * 2\nThe graph tensor has name: Reshape_2:0"
     ]
    }
   ],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=6, restore_best_weights=True)\n",
    "lr_on_plateau = tf.keras.callbacks.ReduceLROnPlateau(patience=3, factor=0.5, min_lr=1e-5, verbose=1)\n",
    "\n",
    "history = tabnet.fit(train_dataset, epochs=100, validation_data=val_dataset, callbacks=[early_stopping, lr_on_plateau])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot history loss and RMSE for training and validation set; train solid line, validation dashed line\n",
    "fig, (top_ax, bottom_ax) = plt.subplots(2, 1, figsize=(10, 10), sharex=True)\n",
    "\n",
    "top_ax.plot(history.history['loss'], label='train_loss', c='b')\n",
    "top_ax.plot(history.history['val_loss'], label='val_loss', linestyle='--', c='b')\n",
    "second_ax = top_ax.twinx()\n",
    "second_ax.plot(history.history['root_mean_squared_error'], label='train_rmse', c='orange')\n",
    "second_ax.plot(history.history['val_root_mean_squared_error'], label='val_rmse', linestyle='--', c='orange')\n",
    "top_ax.set_ylabel('loss')\n",
    "second_ax.set_ylabel('RMSE')\n",
    "top_ax.legend()\n",
    "second_ax.legend()\n",
    "\n",
    "\n",
    "bottom_ax.plot(history.history['lr'], label='lr', c='g')\n",
    "bottom_ax.set_xlabel('Epoch')\n",
    "bottom_ax.set_ylabel('Learning rate')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test set\n",
    "tabnet.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabnet",
   "language": "python",
   "name": "tabnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
