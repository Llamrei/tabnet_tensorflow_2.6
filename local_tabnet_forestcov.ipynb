{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# from tensorflow.python.framework.ops import disable_eager_execution \n",
    "# disable_eager_execution()\n",
    "# from tensorflow.python.framework.ops import enable_eager_execution\n",
    "# enable_eager_execution()\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import common tensorflow layers and activations\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Layer\n",
    "from tensorflow.keras.layers import Lambda, Multiply, Add, Rescaling\n",
    "from tensorflow.keras.activations import relu, sigmoid, softmax\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "from local_tabnet import TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16384\n",
    "LAMBDA = 0.0001\n",
    "N_A = 64\n",
    "N_D = 64\n",
    "VIRTUAL_BATCH_SIZE = 100\n",
    "BATCH_MOMENTUM = 0.7\n",
    "N_STEPS = 5\n",
    "GAMMA = 1.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Defn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  16384\n",
      "Total iterations:  50000\n",
      "Iterations per epoch:  24\n",
      "Max epochs:  2083\n",
      "Iterations to change lr:  500\n",
      "Epochs to change lr:  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-22 16:33:27.669791: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-05-22 16:33:27.669856: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (al3615): /proc/driver/nvidia/version does not exist\n",
      "2023-05-22 16:33:27.670837: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(16384, 55), dtype=tf.float64, name=None), TensorSpec(shape=(16384, 1), dtype=tf.int64, name=None)) (TensorSpec(shape=(16384, 55), dtype=tf.float64, name=None), TensorSpec(shape=(16384, 1), dtype=tf.int64, name=None)) (TensorSpec(shape=(None, 55), dtype=tf.float64, name=None), TensorSpec(shape=(None, 1), dtype=tf.int64, name=None))\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'forestcov'\n",
    "data = pd.read_csv('covtype.data')\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.3)\n",
    "\n",
    "wilderness_areas = [f\"Wilderness_Area{i}\" for i in range(1, 5)]\n",
    "soil_types = [f\"Soil_Type{i}\" for i in range(1, 41)]\n",
    "covariates = [\n",
    "    \"Elevation\", \"Aspect\", \"Slope\", \"Horizontal_Distance_To_Hydrology\",\n",
    "    \"Vertical_Distance_To_Hydrology\", \"Horizontal_Distance_To_Roadways\",\n",
    "    \"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\",\n",
    "    \"Horizontal_Distance_To_Fire_Points\",\n",
    "    *wilderness_areas, *soil_types\n",
    "]\n",
    "target = \"Cover_Type\"\n",
    "features = covariates + [target]\n",
    "train.columns = features\n",
    "test.columns = train.columns\n",
    "\n",
    "iterations_per_epoch = train.shape[0]//BATCH_SIZE\n",
    "total_iterations = 50000\n",
    "iterations_to_change_lr = 500\n",
    "max_epochs = total_iterations//iterations_per_epoch\n",
    "epochs_to_change_lr = iterations_to_change_lr//iterations_per_epoch\n",
    "\n",
    "print(\"Batch size: \", BATCH_SIZE)\n",
    "print(\"Total iterations: \", total_iterations)\n",
    "print(\"Iterations per epoch: \", iterations_per_epoch)\n",
    "print(\"Max epochs: \", max_epochs)\n",
    "print(\"Iterations to change lr: \", iterations_to_change_lr)\n",
    "print(\"Epochs to change lr: \", epochs_to_change_lr)\n",
    "\n",
    "# Split into X and y\n",
    "train_indices, val_indices = train_test_split( np.arange(len(train)), test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = train[features].values[train_indices]\n",
    "y_train = train[target].values[train_indices]\n",
    "\n",
    "X_val = train[features].values[val_indices]\n",
    "y_val = train[target].values[val_indices]\n",
    "\n",
    "X_test = test[features].values\n",
    "y_test = test[target].values\n",
    "\n",
    "# Make tensorflow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train.astype(float), y_train[..., np.newaxis]))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=4048).batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val.astype(float), y_val[..., np.newaxis]))\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test.astype(float), y_test[..., np.newaxis]))\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "print(train_dataset.element_spec, val_dataset.element_spec, test_dataset.element_spec)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model creation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "local_implementation = TabNet(\n",
    "    dim_features=X_train.shape[1],\n",
    "    dim_attention=N_A, \n",
    "    dim_output=10, \n",
    "    sparsity=LAMBDA,\n",
    "    num_steps=N_STEPS,\n",
    "    gamma=GAMMA,  \n",
    "    output_activation=None)\n",
    "\n",
    "# Exponential decay learning rate\n",
    "lr = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-2,\n",
    "    decay_steps=iterations_to_change_lr,\n",
    "    decay_rate=0.95,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "local_implementation.compile( \n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=lr), \n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "local_implementation.build(X_train[:1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-22 16:33:35.384377: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 60s 3s/step - loss: 1.3233 - accuracy: 0.5293 - val_loss: 0.0000e+00 - val_accuracy: 0.4855\n",
      "Epoch 2/1000\n",
      "15/19 [======================>.......] - ETA: 10s - loss: 0.9268 - accuracy: 0.6058"
     ]
    }
   ],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=30, restore_best_weights=True, monitor=\"val_accuracy\")\n",
    "history = local_implementation.fit(train_dataset, epochs=1000, validation_data=val_dataset, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot history loss and RMSE for training and validation set; train solid line, validation dashed line\n",
    "fig, (top_ax) = plt.subplots(1,1, figsize=(10, 10), sharex=True)\n",
    "\n",
    "hist = local_implementation.history.history\n",
    "\n",
    "top_ax.plot(hist['loss'], label='train_loss', c='b')\n",
    "second_ax = top_ax.twinx()\n",
    "# Plot precision and recall on second axis in orange and red respectively\n",
    "second_ax.plot(hist['accuracy'], label='train_accuracy', linestyle='--', c='b')\n",
    "second_ax.plot(hist['val_accuracy'], label='val_accuracy', linestyle='--', c='orange')\n",
    "top_ax.set_ylabel('loss')\n",
    "second_ax.set_ylabel('Accuracy')\n",
    "# Merge top ax legend entries\n",
    "handles, labels = top_ax.get_legend_handles_labels()\n",
    "handles2, labels2 = second_ax.get_legend_handles_labels()\n",
    "top_ax.legend(handles + handles2, labels + labels2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test set\n",
    "local_implementation.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabnet",
   "language": "python",
   "name": "tabnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
