{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Seems definitely wrong\n",
    "class WeightDecayModel(tf.keras.Model):\n",
    "    def __init__(self, weight_decay, *args, **kwargs):\n",
    "        super(WeightDecayModel, self).__init__(*args, **kwargs)\n",
    "        self.weight_decay = weight_decay\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        if training:\n",
    "            l1_penalty = tf.add_n(tf.reduce_sum(x) for x in self.trainable_variables)\n",
    "            self.add_loss(l1_penalty * self.weight_decay)\n",
    "        return super(WeightDecayModel, self).call(inputs, training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seems more plausible\n",
    "class WeightDecayModel(tf.keras.Model):\n",
    "    def __init__(self, weight_decay, *args, **kwargs):\n",
    "        super(WeightDecayModel, self).__init__(*args, **kwargs)\n",
    "        self.weight_decay = tf.convert_to_tensor(weight_decay)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)\n",
    "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        for var in trainable_vars:\n",
    "            # Why am I multiplying by learning rate?\n",
    "            # Because \n",
    "            var.assign_sub(self.optimizer.lr * self.weight_decay * var)\n",
    "        return {\"loss\": loss}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
